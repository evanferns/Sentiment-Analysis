{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "df=pd.read_csv(r'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "for f in df['tweet']:\n",
    "    sentences.append(f)\n",
    "\n",
    "labels=[]\n",
    "for f in df['label']:\n",
    "    labels.append(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                              tweet\n",
      "0      31963  #studiolife #aislife #requires #passion #dedic...\n",
      "1      31964   @user #white #supremacists want everyone to s...\n",
      "2      31965  safe ways to heal your #acne!!    #altwaystohe...\n",
      "3      31966  is the hp and the cursed child book up for res...\n",
      "4      31967    3rd #bihday to my amazing, hilarious #nephew...\n",
      "...      ...                                                ...\n",
      "17192  49155  thought factory: left-right polarisation! #tru...\n",
      "17193  49156  feeling like a mermaid ð #hairflip #neverre...\n",
      "17194  49157  #hillary #campaigned today in #ohio((omg)) &am...\n",
      "17195  49158  happy, at work conference: right mindset leads...\n",
      "17196  49159  my   song \"so glad\" free download!  #shoegaze ...\n",
      "\n",
      "[17197 rows x 2 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'few', 'for', 'hers', 'was', 'isn', 'it', 'am', 'further', 'own', \"won't\", 'wouldn', 'but', 'some', 'does', 's', \"you're\", 'is', 'they', 'before', 'this', 'd', 'aren', \"weren't\", 'your', \"you'd\", 'ourselves', 'that', 'can', 'itself', 'myself', \"don't\", 'having', 'against', 'of', 'when', 'as', 're', \"she's\", 'where', 'doing', 'them', 'and', 'there', 'ma', 'm', 'me', 'now', 've', 'until', 'don', 'very', 'nor', \"hadn't\", 'by', \"needn't\", 'here', 'our', 'ain', 'only', \"hasn't\", 'weren', 'below', 'while', \"wouldn't\", 'were', 'with', 'won', \"shan't\", 'the', 'about', 'o', 'above', \"should've\", 'will', 'shan', 'yourself', 'hasn', 'him', \"doesn't\", 'ours', 'any', 'didn', 'who', 'off', 'after', 'why', 'shouldn', 'under', 'from', 'at', 'have', 'yourselves', 'more', 'hadn', \"mightn't\", 'do', 'needn', 'which', 'theirs', 'an', \"isn't\", \"wasn't\", 'doesn', \"you've\", 'did', 'on', 't', 'up', 'other', 'through', \"shouldn't\", 'you', 'he', 'each', \"mustn't\", 'if', 'between', 'during', 'in', 'y', 'no', 'she', 'wasn', 'a', \"that'll\", 'll', 'its', 'their', 'haven', 'himself', 'so', 'herself', 'mustn', 'same', 'again', 'because', 'then', 'mightn', \"it's\", 'just', 'yours', 'has', \"you'll\", 'to', 'my', 'are', 'couldn', 'these', 'or', 'over', 'had', 'such', 'out', \"couldn't\", 'her', 'being', 'his', 'than', 'those', 'down', 'into', 'most', 'all', \"didn't\", 'what', 'whom', 'i', 'we', 'themselves', 'once', 'not', 'too', 'be', 'been', \"aren't\", \"haven't\", 'both', 'how', 'should'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword=set(stopwords.words('english'))\n",
    "print(stopword)\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '',string.punctuation))\n",
    "def remove_stopwords(text):\n",
    "    new_word=[]\n",
    "    for word in str(text).split():\n",
    "        if word not in stopword:\n",
    "            new_word.append(word)\n",
    "    return \" \".join(new_word)\n",
    "def remove_url(text):\n",
    "    url_pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'',text)\n",
    "def remove_extra(text):\n",
    "    new_word=[]\n",
    "    for word in str(text).split():\n",
    "        if word.isalpha():\n",
    "                    new_word.append(word)\n",
    "    return  \" \".join(new_word)        \n",
    "\n",
    "for i in range (len(sentences)):\n",
    "    \n",
    "    sentences[i]=lowercase(sentences[i])\n",
    "    sentences[i]=remove_url(sentences[i])\n",
    "    sentences[i]=remove_stopwords(sentences[i])\n",
    "    sentences[i]=remove_punctuation(sentences[i])\n",
    "    sentences[i]=remove_extra(sentences[i])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user cnn calls michigan middle school build wall chant tcot\n"
     ]
    }
   ],
   "source": [
    "print(sentences[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=31962\n",
    "seq_len=70\n",
    "pad='post'\n",
    "trunc='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "training_sentences= sentences[0:25000]\n",
    "training_labels=labels[0:25000]\n",
    "\n",
    "test_sentences=sentences[25000:]\n",
    "test_labels=labels[25000:]\n",
    "\n",
    "\n",
    "tokenizer=Tokenizer(num_words=5000,oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "#print(word_index)\n",
    "\n",
    "training_sequences=tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=seq_len, padding=pad, truncating=trunc)\n",
    "#print(training_padded[1])\n",
    "testing_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=seq_len, padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "782/782 - 1s - loss: 0.2923 - accuracy: 0.9291 - val_loss: 0.2457 - val_accuracy: 0.9299\n",
      "Epoch 2/30\n",
      "782/782 - 1s - loss: 0.2338 - accuracy: 0.9298 - val_loss: 0.2218 - val_accuracy: 0.9299\n",
      "Epoch 3/30\n",
      "782/782 - 1s - loss: 0.1892 - accuracy: 0.9318 - val_loss: 0.1635 - val_accuracy: 0.9379\n",
      "Epoch 4/30\n",
      "782/782 - 1s - loss: 0.1272 - accuracy: 0.9506 - val_loss: 0.1364 - val_accuracy: 0.9479\n",
      "Epoch 5/30\n",
      "782/782 - 1s - loss: 0.1008 - accuracy: 0.9624 - val_loss: 0.1321 - val_accuracy: 0.9537\n",
      "Epoch 6/30\n",
      "782/782 - 1s - loss: 0.0895 - accuracy: 0.9666 - val_loss: 0.1309 - val_accuracy: 0.9540\n",
      "Epoch 7/30\n",
      "782/782 - 1s - loss: 0.0821 - accuracy: 0.9703 - val_loss: 0.1320 - val_accuracy: 0.9560\n",
      "Epoch 8/30\n",
      "782/782 - 1s - loss: 0.0754 - accuracy: 0.9730 - val_loss: 0.1366 - val_accuracy: 0.9553\n",
      "Epoch 9/30\n",
      "782/782 - 1s - loss: 0.0697 - accuracy: 0.9749 - val_loss: 0.1367 - val_accuracy: 0.9550\n",
      "Epoch 10/30\n",
      "782/782 - 1s - loss: 0.0653 - accuracy: 0.9775 - val_loss: 0.1417 - val_accuracy: 0.9552\n",
      "Epoch 11/30\n",
      "782/782 - 1s - loss: 0.0614 - accuracy: 0.9789 - val_loss: 0.1466 - val_accuracy: 0.9553\n",
      "Epoch 12/30\n",
      "782/782 - 1s - loss: 0.0576 - accuracy: 0.9804 - val_loss: 0.1493 - val_accuracy: 0.9527\n",
      "Epoch 13/30\n",
      "782/782 - 1s - loss: 0.0546 - accuracy: 0.9819 - val_loss: 0.1562 - val_accuracy: 0.9548\n",
      "Epoch 14/30\n",
      "782/782 - 1s - loss: 0.0516 - accuracy: 0.9830 - val_loss: 0.1686 - val_accuracy: 0.9562\n",
      "Epoch 15/30\n",
      "782/782 - 1s - loss: 0.0491 - accuracy: 0.9835 - val_loss: 0.1671 - val_accuracy: 0.9546\n",
      "Epoch 16/30\n",
      "782/782 - 1s - loss: 0.0466 - accuracy: 0.9844 - val_loss: 0.1730 - val_accuracy: 0.9512\n",
      "Epoch 17/30\n",
      "782/782 - 1s - loss: 0.0440 - accuracy: 0.9852 - val_loss: 0.1793 - val_accuracy: 0.9517\n",
      "Epoch 18/30\n",
      "782/782 - 1s - loss: 0.0421 - accuracy: 0.9860 - val_loss: 0.1854 - val_accuracy: 0.9532\n",
      "Epoch 19/30\n",
      "782/782 - 1s - loss: 0.0401 - accuracy: 0.9871 - val_loss: 0.1991 - val_accuracy: 0.9539\n",
      "Epoch 20/30\n",
      "782/782 - 1s - loss: 0.0385 - accuracy: 0.9873 - val_loss: 0.2051 - val_accuracy: 0.9527\n",
      "Epoch 21/30\n",
      "782/782 - 1s - loss: 0.0365 - accuracy: 0.9881 - val_loss: 0.2086 - val_accuracy: 0.9506\n",
      "Epoch 22/30\n",
      "782/782 - 1s - loss: 0.0353 - accuracy: 0.9881 - val_loss: 0.2211 - val_accuracy: 0.9523\n",
      "Epoch 23/30\n",
      "782/782 - 1s - loss: 0.0332 - accuracy: 0.9898 - val_loss: 0.2251 - val_accuracy: 0.9516\n",
      "Epoch 24/30\n",
      "782/782 - 1s - loss: 0.0319 - accuracy: 0.9900 - val_loss: 0.2448 - val_accuracy: 0.9537\n",
      "Epoch 25/30\n",
      "782/782 - 1s - loss: 0.0312 - accuracy: 0.9896 - val_loss: 0.2533 - val_accuracy: 0.9543\n",
      "Epoch 26/30\n",
      "782/782 - 1s - loss: 0.0297 - accuracy: 0.9904 - val_loss: 0.2518 - val_accuracy: 0.9503\n",
      "Epoch 27/30\n",
      "782/782 - 1s - loss: 0.0282 - accuracy: 0.9910 - val_loss: 0.2628 - val_accuracy: 0.9509\n",
      "Epoch 28/30\n",
      "782/782 - 1s - loss: 0.0276 - accuracy: 0.9911 - val_loss: 0.2713 - val_accuracy: 0.9509\n",
      "Epoch 29/30\n",
      "782/782 - 1s - loss: 0.0271 - accuracy: 0.9910 - val_loss: 0.2768 - val_accuracy: 0.9489\n",
      "Epoch 30/30\n",
      "782/782 - 1s - loss: 0.0258 - accuracy: 0.9918 - val_loss: 0.2890 - val_accuracy: 0.9384\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_size,8,input_length=seq_len),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(training_padded, training_labels, epochs=25, validation_data=(testing_padded, test_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                              tweet\n",
      "0      31963  #studiolife #aislife #requires #passion #dedic...\n",
      "1      31964   @user #white #supremacists want everyone to s...\n",
      "2      31965  safe ways to heal your #acne!!    #altwaystohe...\n",
      "3      31966  is the hp and the cursed child book up for res...\n",
      "4      31967    3rd #bihday to my amazing, hilarious #nephew...\n",
      "...      ...                                                ...\n",
      "17192  49155  thought factory: left-right polarisation! #tru...\n",
      "17193  49156  feeling like a mermaid ð #hairflip #neverre...\n",
      "17194  49157  #hillary #campaigned today in #ohio((omg)) &am...\n",
      "17195  49158  happy, at work conference: right mindset leads...\n",
      "17196  49159  my   song \"so glad\" free download!  #shoegaze ...\n",
      "\n",
      "[17197 rows x 2 columns]\n",
      "17197\n"
     ]
    }
   ],
   "source": [
    "d=pd.read_csv(r'test.csv')\n",
    "print(d)\n",
    "\n",
    "test_sentences=[]\n",
    "for f in d['tweet']:\n",
    "    test_sentences.append(f)\n",
    "print(len(test_sentences))\n",
    "for i in range (len(test_sentences)):\n",
    "    \n",
    "    test_sentences[i]=lowercase(test_sentences[i])\n",
    "    test_sentences[i]=remove_url(test_sentences[i])\n",
    "    test_sentences[i]=remove_stopwords(test_sentences[i])\n",
    "    test_sentences[i]=remove_punctuation(test_sentences[i])\n",
    "    test_sentences[i]=remove_extra(test_sentences[i])    \n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=seq_len, padding=pad, truncating=pad)\n",
    "\n",
    "test_padded = np.array(test_padded)\n",
    "\n",
    "p=model.predict(test_padded)\n",
    "\n",
    "d['labels']=p\n",
    "d.to_csv(\"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
